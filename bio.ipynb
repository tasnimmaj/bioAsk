{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Ce projet se concentre sur la cr√©ation d'un mod√®le de question-r√©ponse sp√©cialis√© dans le domaine biom√©dical, bas√© sur l'architecture BERT (Bidirectional Encoder Representations from Transformers). En utilisant le jeu de donn√©es bioASK, le mod√®le est con√ßu pour comprendre des questions en langage naturel et trouver des r√©ponses sp√©cifiques dans un texte fourni. La d√©marche suit plusieurs √©tapes essentielles, depuis la pr√©paration des donn√©es jusqu‚Äô√† l'entra√Ænement et la sauvegarde du mod√®le, afin de garantir sa pr√©cision dans le domaine m√©dical. Ce projet d√©montre √©galement l'int√©gration de biblioth√®ques modernes comme Hugging Face Transformers et TensorFlow pour le d√©veloppement d'un mod√®le performant de question-r√©ponse, ouvrant des perspectives dans la cr√©ation d'outils d‚Äôaide √† la recherche d'information sp√©cialis√©e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation du Mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertTokenizerFast, TFBertForQuestionAnswering\n",
    "import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "from transformers import TFBertForQuestionAnswering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La premi√®re √©tape consiste √† importer les modules n√©cessaires pour le projet, en particulier Hugging Face Transformers pour le mod√®le BERT et le tokenizer, ainsi que TensorFlow pour la cr√©ation et l'entra√Ænement du mod√®le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le fichier JSON de BIOAsk\n",
    "with open(\"bio.json\") as f:\n",
    "    squad_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On initialise le tokenizer BERT, qui d√©coupe les phrases en tokens que le mod√®le peut interpr√©ter. Ce tokenizer est utilis√© pour transformer les questions et les contextes en vecteurs de tokens exploitables par BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de questions : 4772\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Pr√©paration des donn√©es (extrait question et contexte)\n",
    "questions = []\n",
    "contexts = []\n",
    "answers = []\n",
    "\n",
    "for article in squad_data[\"data\"]:\n",
    "    for paragraph in article[\"paragraphs\"]:\n",
    "        context = paragraph[\"context\"]\n",
    "        for qa in paragraph[\"qas\"]:\n",
    "            question = qa[\"question\"]\n",
    "            answer = qa[\"answers\"][0][\"text\"]\n",
    "            questions.append(question)\n",
    "            contexts.append(context)\n",
    "            answers.append(answer)\n",
    "\n",
    "# Affichage du nombre d'entr√©es\n",
    "print(f\"Nombre de questions : {len(questions)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pr√©paration des Donn√©es pour le Mod√®le\n",
    "Cette section tokenize les questions et contextes et d√©termine les positions de d√©but et de fin des r√©ponses. Cela permet de sp√©cifier l‚Äôemplacement exact des r√©ponses dans le contexte, essentiel pour l‚Äôapprentissage supervis√© du mod√®le.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pr√©paration des donn√©es\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "questions = []\n",
    "contexts = []\n",
    "start_positions = []\n",
    "end_positions = []\n",
    "\n",
    "for article in squad_data[\"data\"]:\n",
    "    for paragraph in article[\"paragraphs\"]:\n",
    "        context = paragraph[\"context\"]\n",
    "        for qa in paragraph[\"qas\"]:\n",
    "            question = qa[\"question\"]\n",
    "            answer = qa[\"answers\"][0]\n",
    "            answer_text = answer[\"text\"]\n",
    "            start_idx = answer[\"answer_start\"]\n",
    "            end_idx = start_idx + len(answer_text)\n",
    "\n",
    "            # Tokenisation avec le tokenizer rapide\n",
    "            inputs = tokenizer(question, context, return_tensors=\"tf\", truncation=True, padding=\"max_length\", max_length=384)\n",
    "\n",
    "            # Obtenir les positions de d√©but et de fin\n",
    "            start_pos = inputs.char_to_token(0, start_idx)\n",
    "            end_pos = inputs.char_to_token(0, end_idx - 1)\n",
    "            \n",
    "            # V√©rifier si les positions ne sont pas None\n",
    "            if start_pos is not None and end_pos is not None:\n",
    "                start_positions.append(start_pos)\n",
    "                end_positions.append(end_pos)\n",
    "                questions.append(question)\n",
    "                contexts.append(context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion des Donn√©es en Tenseurs\n",
    "Une fois les positions de d√©but et de fin √©tablies, elles sont converties en tenseurs TensorFlow, afin d‚Äô√™tre pr√™tes pour l‚Äôentra√Ænement du mod√®le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Conversion en tenseurs pour l'entra√Ænement\n",
    "start_positions = tf.convert_to_tensor(start_positions)\n",
    "end_positions = tf.convert_to_tensor(end_positions)\n",
    "inputs = tokenizer(questions, contexts, return_tensors=\"tf\", padding=True, truncation=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement , Compilation et Entra√Ænement du Mod√®le\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, le mod√®le TFBertForQuestionAnswering est configur√© pour une t√¢che de question-r√©ponse. Il est compil√© avec l‚Äôoptimiseur Adam et une fonction de perte pour les positions de d√©but et de fin, assurant une optimisation correcte pendant l‚Äôentra√Ænement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Charger le mod√®le\n",
    "model = TFBertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForQuestionAnswering were not initialized from the PyTorch model and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "WARNING:tensorflow:From c:\\Users\\MAJOUL\\ai BERT\\.conda\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\MAJOUL\\ai BERT\\.conda\\Lib\\site-packages\\tf_keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "34/34 [==============================] - 600s 17s/step - loss: 6.7903 - start_logits_loss: 6.7903\n",
      "Epoch 2/4\n",
      "34/34 [==============================] - 644s 19s/step - loss: 6.2383 - start_logits_loss: 6.2383\n",
      "Epoch 3/4\n",
      "34/34 [==============================] - 618s 18s/step - loss: 6.2383 - start_logits_loss: 6.2383\n",
      "Epoch 4/4\n",
      "34/34 [==============================] - 481s 14s/step - loss: 6.2383 - start_logits_loss: 6.2383\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x262543ecc10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Compiler le mod√®le\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),\n",
    "              loss={'start_positions': 'sparse_categorical_crossentropy', 'end_positions': 'sparse_categorical_crossentropy'})\n",
    "\n",
    "# D√©finir un callback pour sauvegarder le mod√®le √† chaque epoch\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"model_epoch_{epoch}.h5\",  # Nom du fichier avec num√©ro d'epoch\n",
    "    save_weights_only=True,              # Sauvegarder uniquement les poids\n",
    "    save_freq='epoch'                    # Sauvegarder √† chaque epoch\n",
    ")\n",
    "\n",
    "# Entra√Ænement avec le callback\n",
    "model.fit(inputs, {\"start_positions\": start_positions, \"end_positions\": end_positions},\n",
    "          epochs=4, batch_size=8, callbacks=[checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code de l'application streamlit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Charger le mod√®le et le tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "model = TFBertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Charger le fichier JSON de donn√©es\n",
    "with open(\"bio.json\") as f:\n",
    "    squad_data = json.load(f)\n",
    "\n",
    "# Fonction pour obtenir la r√©ponse pr√©dite par le mod√®le\n",
    "def get_predicted_answer(question, context):\n",
    "    # Tokenizer en gardant une longueur maximale plus flexible\n",
    "    inputs = tokenizer(question, context, return_tensors=\"tf\", truncation=True, padding=\"max_length\", max_length=512)\n",
    "    \n",
    "    # Pr√©diction des logits de d√©but et de fin\n",
    "    outputs = model(inputs)\n",
    "    start_logits = outputs.start_logits\n",
    "    end_logits = outputs.end_logits\n",
    "\n",
    "    # Trouver les positions des tokens de d√©but et de fin avec les scores les plus √©lev√©s\n",
    "    start_idx = tf.argmax(start_logits, axis=1).numpy()[0]\n",
    "    end_idx = tf.argmax(end_logits, axis=1).numpy()[0] + 1\n",
    "\n",
    "    # V√©rification si la position de fin est apr√®s celle du d√©but\n",
    "    if end_idx < start_idx:\n",
    "        end_idx = start_idx + 1\n",
    "\n",
    "    # Convertir les tokens en texte\n",
    "    answer_tokens = inputs[\"input_ids\"][0][start_idx:end_idx]\n",
    "    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
    "\n",
    "    # Nettoyage pour √©viter les erreurs dans la r√©ponse\n",
    "    if len(answer.strip()) == 0:\n",
    "        answer = \"Je ne peux pas trouver une r√©ponse appropri√©e dans le contexte fourni.\"\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Interface Streamlit\n",
    "st.set_page_config(page_title=\"Medical Question-Answering Bot\", page_icon=\"ü§ñ\")\n",
    "st.title(\"Medical Question-Answering Bot\")\n",
    "\n",
    "# Message dans la barre lat√©rale\n",
    "with st.sidebar:\n",
    "    st.title(\"Param√®tres\")\n",
    "    st.write(\"Entrez le contexte et la question pour obtenir une r√©ponse bas√©e sur le mod√®le de QA.\")\n",
    "\n",
    "# Prendre les entr√©es de l'utilisateur\n",
    "user_context = st.text_area(\"Entrez le texte ou le contexte ici...\")\n",
    "user_question = st.text_input(\"Tapez votre question ici...\")\n",
    "\n",
    "# Si le contexte et la question sont fournis, obtenir la r√©ponse\n",
    "if user_context and user_question:\n",
    "    predicted_answer = get_predicted_answer(user_question, user_context)\n",
    "\n",
    "    # Afficher la r√©ponse pr√©dite\n",
    "    st.write(f\"R√©ponse : {predicted_answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Ce projet propose une m√©thode compl√®te pour d√©velopper un syst√®me de question-r√©ponse biom√©dical, bas√© sur BERT, adapt√© aux besoins de recherche de r√©ponses pr√©cises dans des corpus sp√©cialis√©s. Gr√¢ce √† un processus structur√©, allant de la pr√©paration des donn√©es √† l'entra√Ænement du mod√®le, ce syst√®me d√©montre l'efficacit√© de l'apprentissage profond dans les applications de traitement du langage naturel. Pour l'avenir, une exploration d'autres architectures plus avanc√©es ou des optimisations sp√©cifiques au domaine m√©dical pourraient encore am√©liorer les performances, rendant cet outil d'autant plus utile pour les chercheurs, les m√©decins et les √©tudiants en sciences de la sant√©."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#R√©f√©rences \n",
    "\n",
    "\n",
    "[1] BERT for Question Answering on BioASQ (2020)\n",
    "Auteurs: Fu, Djoko, Mansor, and Slater\n",
    "\n",
    "\n",
    "[2] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018) \n",
    "Auteurs: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova\n",
    "\n",
    "\n",
    "\n",
    "[3]BERT for Extractive Question Answering on SQuAD(2018)\n",
    "Auteurs: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova\n",
    "\n",
    "\n",
    "[4]Transformers for Biomedical Question Answering: A Systematic Review (2021)\n",
    "Auteurs:Y. Rasmy, J. Li, and M. Liu\n",
    "\n",
    "\n",
    "[5]Latent Structure in Transformer-based QA Systems (2020)\n",
    "Auteurs:des chercheurs sp√©cialis√©s dans les mod√®les de transformers\n",
    "\n",
    "[6]GitHub - BERT BioASQ Q/A Mahdiar-Khodabakhshi\n",
    "\n",
    "[7]Kaggle - BioASQ Dataset\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
