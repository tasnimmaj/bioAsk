{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Ce projet se concentre sur la création d'un modèle de question-réponse spécialisé dans le domaine biomédical, basé sur l'architecture BERT (Bidirectional Encoder Representations from Transformers). En utilisant le jeu de données bioASK, le modèle est conçu pour comprendre des questions en langage naturel et trouver des réponses spécifiques dans un texte fourni. La démarche suit plusieurs étapes essentielles, depuis la préparation des données jusqu’à l'entraînement et la sauvegarde du modèle, afin de garantir sa précision dans le domaine médical. Ce projet démontre également l'intégration de bibliothèques modernes comme Hugging Face Transformers et TensorFlow pour le développement d'un modèle performant de question-réponse, ouvrant des perspectives dans la création d'outils d’aide à la recherche d'information spécialisée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation du Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertTokenizerFast, TFBertForQuestionAnswering\n",
    "import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "from transformers import TFBertForQuestionAnswering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La première étape consiste à importer les modules nécessaires pour le projet, en particulier Hugging Face Transformers pour le modèle BERT et le tokenizer, ainsi que TensorFlow pour la création et l'entraînement du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le fichier JSON de BIOAsk\n",
    "with open(\"bio.json\") as f:\n",
    "    squad_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On initialise le tokenizer BERT, qui découpe les phrases en tokens que le modèle peut interpréter. Ce tokenizer est utilisé pour transformer les questions et les contextes en vecteurs de tokens exploitables par BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de questions : 4772\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Préparation des données (extrait question et contexte)\n",
    "questions = []\n",
    "contexts = []\n",
    "answers = []\n",
    "\n",
    "for article in squad_data[\"data\"]:\n",
    "    for paragraph in article[\"paragraphs\"]:\n",
    "        context = paragraph[\"context\"]\n",
    "        for qa in paragraph[\"qas\"]:\n",
    "            question = qa[\"question\"]\n",
    "            answer = qa[\"answers\"][0][\"text\"]\n",
    "            questions.append(question)\n",
    "            contexts.append(context)\n",
    "            answers.append(answer)\n",
    "\n",
    "# Affichage du nombre d'entrées\n",
    "print(f\"Nombre de questions : {len(questions)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préparation des Données pour le Modèle\n",
    "Cette section tokenize les questions et contextes et détermine les positions de début et de fin des réponses. Cela permet de spécifier l’emplacement exact des réponses dans le contexte, essentiel pour l’apprentissage supervisé du modèle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Préparation des données\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "questions = []\n",
    "contexts = []\n",
    "start_positions = []\n",
    "end_positions = []\n",
    "\n",
    "for article in squad_data[\"data\"]:\n",
    "    for paragraph in article[\"paragraphs\"]:\n",
    "        context = paragraph[\"context\"]\n",
    "        for qa in paragraph[\"qas\"]:\n",
    "            question = qa[\"question\"]\n",
    "            answer = qa[\"answers\"][0]\n",
    "            answer_text = answer[\"text\"]\n",
    "            start_idx = answer[\"answer_start\"]\n",
    "            end_idx = start_idx + len(answer_text)\n",
    "\n",
    "            # Tokenisation avec le tokenizer rapide\n",
    "            inputs = tokenizer(question, context, return_tensors=\"tf\", truncation=True, padding=\"max_length\", max_length=384)\n",
    "\n",
    "            # Obtenir les positions de début et de fin\n",
    "            start_pos = inputs.char_to_token(0, start_idx)\n",
    "            end_pos = inputs.char_to_token(0, end_idx - 1)\n",
    "            \n",
    "            # Vérifier si les positions ne sont pas None\n",
    "            if start_pos is not None and end_pos is not None:\n",
    "                start_positions.append(start_pos)\n",
    "                end_positions.append(end_pos)\n",
    "                questions.append(question)\n",
    "                contexts.append(context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion des Données en Tenseurs\n",
    "Une fois les positions de début et de fin établies, elles sont converties en tenseurs TensorFlow, afin d’être prêtes pour l’entraînement du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Conversion en tenseurs pour l'entraînement\n",
    "start_positions = tf.convert_to_tensor(start_positions)\n",
    "end_positions = tf.convert_to_tensor(end_positions)\n",
    "inputs = tokenizer(questions, contexts, return_tensors=\"tf\", padding=True, truncation=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement , Compilation et Entraînement du Modèle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, le modèle TFBertForQuestionAnswering est configuré pour une tâche de question-réponse. Il est compilé avec l’optimiseur Adam et une fonction de perte pour les positions de début et de fin, assurant une optimisation correcte pendant l’entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Charger le modèle\n",
    "model = TFBertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForQuestionAnswering were not initialized from the PyTorch model and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "WARNING:tensorflow:From c:\\Users\\MAJOUL\\ai BERT\\.conda\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\MAJOUL\\ai BERT\\.conda\\Lib\\site-packages\\tf_keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "34/34 [==============================] - 600s 17s/step - loss: 6.7903 - start_logits_loss: 6.7903\n",
      "Epoch 2/4\n",
      "34/34 [==============================] - 644s 19s/step - loss: 6.2383 - start_logits_loss: 6.2383\n",
      "Epoch 3/4\n",
      "34/34 [==============================] - 618s 18s/step - loss: 6.2383 - start_logits_loss: 6.2383\n",
      "Epoch 4/4\n",
      "34/34 [==============================] - 481s 14s/step - loss: 6.2383 - start_logits_loss: 6.2383\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x262543ecc10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Compiler le modèle\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),\n",
    "              loss={'start_positions': 'sparse_categorical_crossentropy', 'end_positions': 'sparse_categorical_crossentropy'})\n",
    "\n",
    "# Définir un callback pour sauvegarder le modèle à chaque epoch\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"model_epoch_{epoch}.h5\",  # Nom du fichier avec numéro d'epoch\n",
    "    save_weights_only=True,              # Sauvegarder uniquement les poids\n",
    "    save_freq='epoch'                    # Sauvegarder à chaque epoch\n",
    ")\n",
    "\n",
    "# Entraînement avec le callback\n",
    "model.fit(inputs, {\"start_positions\": start_positions, \"end_positions\": end_positions},\n",
    "          epochs=4, batch_size=8, callbacks=[checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code de l'application streamlit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Charger le modèle et le tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "model = TFBertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Charger le fichier JSON de données\n",
    "with open(\"bio.json\") as f:\n",
    "    squad_data = json.load(f)\n",
    "\n",
    "# Fonction pour obtenir la réponse prédite par le modèle\n",
    "def get_predicted_answer(question, context):\n",
    "    # Tokenizer en gardant une longueur maximale plus flexible\n",
    "    inputs = tokenizer(question, context, return_tensors=\"tf\", truncation=True, padding=\"max_length\", max_length=512)\n",
    "    \n",
    "    # Prédiction des logits de début et de fin\n",
    "    outputs = model(inputs)\n",
    "    start_logits = outputs.start_logits\n",
    "    end_logits = outputs.end_logits\n",
    "\n",
    "    # Trouver les positions des tokens de début et de fin avec les scores les plus élevés\n",
    "    start_idx = tf.argmax(start_logits, axis=1).numpy()[0]\n",
    "    end_idx = tf.argmax(end_logits, axis=1).numpy()[0] + 1\n",
    "\n",
    "    # Vérification si la position de fin est après celle du début\n",
    "    if end_idx < start_idx:\n",
    "        end_idx = start_idx + 1\n",
    "\n",
    "    # Convertir les tokens en texte\n",
    "    answer_tokens = inputs[\"input_ids\"][0][start_idx:end_idx]\n",
    "    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
    "\n",
    "    # Nettoyage pour éviter les erreurs dans la réponse\n",
    "    if len(answer.strip()) == 0:\n",
    "        answer = \"Je ne peux pas trouver une réponse appropriée dans le contexte fourni.\"\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Interface Streamlit\n",
    "st.set_page_config(page_title=\"Medical Question-Answering Bot\", page_icon=\"🤖\")\n",
    "st.title(\"Medical Question-Answering Bot\")\n",
    "\n",
    "# Message dans la barre latérale\n",
    "with st.sidebar:\n",
    "    st.title(\"Paramètres\")\n",
    "    st.write(\"Entrez le contexte et la question pour obtenir une réponse basée sur le modèle de QA.\")\n",
    "\n",
    "# Prendre les entrées de l'utilisateur\n",
    "user_context = st.text_area(\"Entrez le texte ou le contexte ici...\")\n",
    "user_question = st.text_input(\"Tapez votre question ici...\")\n",
    "\n",
    "# Si le contexte et la question sont fournis, obtenir la réponse\n",
    "if user_context and user_question:\n",
    "    predicted_answer = get_predicted_answer(user_question, user_context)\n",
    "\n",
    "    # Afficher la réponse prédite\n",
    "    st.write(f\"Réponse : {predicted_answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Ce projet propose une méthode complète pour développer un système de question-réponse biomédical, basé sur BERT, adapté aux besoins de recherche de réponses précises dans des corpus spécialisés. Grâce à un processus structuré, allant de la préparation des données à l'entraînement du modèle, ce système démontre l'efficacité de l'apprentissage profond dans les applications de traitement du langage naturel. Pour l'avenir, une exploration d'autres architectures plus avancées ou des optimisations spécifiques au domaine médical pourraient encore améliorer les performances, rendant cet outil d'autant plus utile pour les chercheurs, les médecins et les étudiants en sciences de la santé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Références \n",
    "\n",
    "\n",
    "[1] BERT for Question Answering on BioASQ (2020)\n",
    "Auteurs: Fu, Djoko, Mansor, and Slater\n",
    "\n",
    "\n",
    "[2] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018) \n",
    "Auteurs: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova\n",
    "\n",
    "\n",
    "\n",
    "[3]BERT for Extractive Question Answering on SQuAD(2018)\n",
    "Auteurs: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova\n",
    "\n",
    "\n",
    "[4]Transformers for Biomedical Question Answering: A Systematic Review (2021)\n",
    "Auteurs:Y. Rasmy, J. Li, and M. Liu\n",
    "\n",
    "\n",
    "[5]Latent Structure in Transformer-based QA Systems (2020)\n",
    "Auteurs:des chercheurs spécialisés dans les modèles de transformers\n",
    "\n",
    "[6]GitHub - BERT BioASQ Q/A Mahdiar-Khodabakhshi\n",
    "\n",
    "[7]Kaggle - BioASQ Dataset\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
